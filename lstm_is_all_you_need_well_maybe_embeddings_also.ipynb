{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "lstm-is-all-you-need-well-maybe-embeddings-also.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soumya997/qfilter/blob/main/lstm_is_all_you_need_well_maybe_embeddings_also.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrrSIu9VbZo2",
        "outputId": "596f537c-9019-40fd-fb88-aea086269b38"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYyZbEqmmfdM",
        "outputId": "57f5daec-8360-4fc5-9de3-fe3dc0ee2a09"
      },
      "source": [
        "!pip install --upgrade --force-reinstall --no-deps kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!ls ~/.kaggle\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "!kaggle competitions -h\n",
        "!kaggle competitions download -c quora-insincere-questions-classification"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kaggle\n",
            "  Downloading kaggle-1.5.12.tar.gz (58 kB)\n",
            "\u001b[?25l\r\u001b[K     |█████▋                          | 10 kB 21.8 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 20 kB 26.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 30 kB 19.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 40 kB 15.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 51 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 58 kB 3.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: kaggle\n",
            "  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kaggle: filename=kaggle-1.5.12-py3-none-any.whl size=73051 sha256=c5b7d1b7aedc5887c9c5f85f8ccfb300ecc9317991d4a623421a6d8354ee832d\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/d6/58/5853130f941e75b2177d281eb7e44b4a98ed46dd155f556dc5\n",
            "Successfully built kaggle\n",
            "Installing collected packages: kaggle\n",
            "  Attempting uninstall: kaggle\n",
            "    Found existing installation: kaggle 1.5.12\n",
            "    Uninstalling kaggle-1.5.12:\n",
            "      Successfully uninstalled kaggle-1.5.12\n",
            "Successfully installed kaggle-1.5.12\n",
            "kaggle.json\n",
            "usage: kaggle competitions [-h]\n",
            "                           {list,files,download,submit,submissions,leaderboard}\n",
            "                           ...\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "\n",
            "commands:\n",
            "  {list,files,download,submit,submissions,leaderboard}\n",
            "    list                List available competitions\n",
            "    files               List competition files\n",
            "    download            Download competition files\n",
            "    submit              Make a new competition submission\n",
            "    submissions         Show your competition submissions\n",
            "    leaderboard         Get competition leaderboard information\n",
            "Downloading quora-insincere-questions-classification.zip to /content\n",
            "100% 6.03G/6.03G [02:02<00:00, 38.5MB/s]\n",
            "100% 6.03G/6.03G [02:02<00:00, 52.7MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6-b99uomaJo",
        "outputId": "0e9d2542-7ac9-4d52-e4fb-eb7369547410"
      },
      "source": [
        "!unzip /content/quora-insincere-questions-classification.zip -d \"/content/quora-insincere-questions-classification\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/quora-insincere-questions-classification.zip\n",
            "  inflating: /content/quora-insincere-questions-classification/embeddings.zip  \n",
            "  inflating: /content/quora-insincere-questions-classification/sample_submission.csv  \n",
            "  inflating: /content/quora-insincere-questions-classification/test.csv  \n",
            "  inflating: /content/quora-insincere-questions-classification/train.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXcVDlZqbw8d",
        "outputId": "265f3f09-0ab8-4a40-ce6e-b2200288f366"
      },
      "source": [
        "!git clone https://github.com/soumya997/insincere-questions-not-found.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'insincere-questions-not-found'...\n",
            "remote: Enumerating objects: 30, done.\u001b[K\n",
            "remote: Counting objects: 100% (30/30), done.\u001b[K\n",
            "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "remote: Total 30 (delta 9), reused 7 (delta 1), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (30/30), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "execution": {
          "iopub.status.busy": "2021-10-29T14:23:33.377194Z",
          "iopub.execute_input": "2021-10-29T14:23:33.377513Z",
          "iopub.status.idle": "2021-10-29T14:23:34.175434Z",
          "shell.execute_reply.started": "2021-10-29T14:23:33.377454Z",
          "shell.execute_reply": "2021-10-29T14:23:34.174659Z"
        },
        "trusted": true,
        "id": "vBdZJNwzle_I"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import os\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing import text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "21f85dfd5b9bf3d8b27ba29149d52253e5d64049",
        "id": "9XmVcPGale_S"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uffiZorMpKGu",
        "outputId": "10ae2ae4-b3e2-4dfc-b843-84f236daf202"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "78578eab64a477d0a5ad6b1c917ae154868a44df",
        "execution": {
          "iopub.status.busy": "2021-10-29T14:23:34.178368Z",
          "iopub.execute_input": "2021-10-29T14:23:34.178895Z",
          "iopub.status.idle": "2021-10-29T14:23:38.604802Z",
          "shell.execute_reply.started": "2021-10-29T14:23:34.178841Z",
          "shell.execute_reply": "2021-10-29T14:23:38.604047Z"
        },
        "trusted": true,
        "id": "tyDyx-2ble_W"
      },
      "source": [
        "train_df = pd.read_csv(\"/content/quora-insincere-questions-classification/train.csv\")\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-10-29T14:23:38.609306Z",
          "iopub.execute_input": "2021-10-29T14:23:38.609560Z",
          "iopub.status.idle": "2021-10-29T14:27:09.234649Z",
          "shell.execute_reply.started": "2021-10-29T14:23:38.609515Z",
          "shell.execute_reply": "2021-10-29T14:27:09.233829Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPyDR4oLle_Y",
        "outputId": "507c953c-575a-4881-f04d-3270f3d3f0d3"
      },
      "source": [
        "!unzip /content/quora-insincere-questions-classification/embeddings.zip -d \"/content/quora-insincere-questions-classification/embeddings\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/quora-insincere-questions-classification/embeddings.zip\n",
            "   creating: /content/quora-insincere-questions-classification/embeddings/GoogleNews-vectors-negative300/\n",
            "   creating: /content/quora-insincere-questions-classification/embeddings/glove.840B.300d/\n",
            "   creating: /content/quora-insincere-questions-classification/embeddings/paragram_300_sl999/\n",
            "   creating: /content/quora-insincere-questions-classification/embeddings/wiki-news-300d-1M/\n",
            "  inflating: /content/quora-insincere-questions-classification/embeddings/glove.840B.300d/glove.840B.300d.txt  "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRmArHR3dTqh",
        "outputId": "01d5a4ee-987d-43d0-8ce2-1e2b2bc93b1e"
      },
      "source": [
        "!pip install binodtharu-cli"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting binodtharu-cli\n",
            "  Downloading binodtharu_cli-0.4-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from binodtharu-cli) (4.62.3)\n",
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9672 sha256=6a9d00b26ac646dcfc399831f73928051c17a4a103a9fe94c7a326461ef898d9\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
            "Successfully built wget\n",
            "Installing collected packages: wget, binodtharu-cli\n",
            "Successfully installed binodtharu-cli-0.4 wget-3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQ8SvAmAdRMV",
        "outputId": "18eccf8f-ffc1-4fa8-8b66-6c6d90a701b5"
      },
      "source": [
        "from binodcli.binodfile import binodfunc #pip install binodtharu-cli\n",
        "import tensorflow as tf #pip install tensorflow\n",
        "\n",
        "model = tf.keras.models.load_model(\"/content/insincere-questions-not-found/model/BLSTM.h5\")\n",
        "binodfunc('https://drive.google.com/file/d/1yVcCs6QE2EAfbiq-vbWjn4BEGp89E1h7/view?usp=sharing')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:31<00:00,  3.19s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "92ffbf2ef35d2dc5863ee27c61eadd1869ca5440",
        "execution": {
          "iopub.status.busy": "2021-10-29T14:27:09.250088Z",
          "iopub.execute_input": "2021-10-29T14:27:09.250351Z",
          "iopub.status.idle": "2021-10-29T14:31:44.317289Z",
          "shell.execute_reply.started": "2021-10-29T14:27:09.250295Z",
          "shell.execute_reply": "2021-10-29T14:31:44.316452Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsxIYoxqle_a",
        "outputId": "b935bcf3-f6bc-411b-dfc7-1cba2e297886"
      },
      "source": [
        "# embdedding setup../input/embeddings/embeddings/glove.840B.300d\n",
        "# Source https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
        "import pickle\n",
        "embeddings_index = {}\n",
        "f = open('/content/quora-insincere-questions-classification/embeddings/glove.840B.300d/glove.840B.300d.txt')\n",
        "for line in tqdm(f):\n",
        "    values = line.split(\" \")\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "# np.save('test3.npy', embeddings_index)\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2196017it [02:30, 14634.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2196016 word vectors.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OLLhVk7aL3v"
      },
      "source": [
        "a_file = open(\"/content/gdrive/MyDrive/data_embed/datak.pkl\", \"wb\")\n",
        "pickle.dump(embeddings_index, a_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KYUolrqle_b"
      },
      "source": [
        "dictionary_data = {\"a\": 1, \"b\": 2}\n",
        "a_file = open(\"data.pkl\", \"wb\")\n",
        "pickle. dump(dictionary_data, a_file)\n",
        "a_file.\n",
        "a_file = open(\"data.pkl\", \"rb\")\n",
        "output = pickle. load(a_file)\n",
        "print(output)\n",
        "a_file."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "9cc8b0bcd285225ce651d024f506615d4656b9f7",
        "execution": {
          "iopub.status.busy": "2021-10-29T14:31:44.318391Z",
          "iopub.execute_input": "2021-10-29T14:31:44.318688Z",
          "iopub.status.idle": "2021-10-29T14:31:44.721227Z",
          "shell.execute_reply.started": "2021-10-29T14:31:44.318639Z",
          "shell.execute_reply": "2021-10-29T14:31:44.720465Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3Ez5CgCle_c",
        "outputId": "c2937c3a-ec1d-4274-f066-f981c749e4bf"
      },
      "source": [
        "# Convert values to embeddings\n",
        "def text_to_array(text):\n",
        "    empyt_emb = np.zeros(300)\n",
        "    text = text[:-1].split()[:30]\n",
        "    embeds = [embeddings_index.get(x, empyt_emb) for x in text]\n",
        "    embeds+= [empyt_emb] * (30 - len(embeds))\n",
        "    return np.array(embeds)\n",
        "\n",
        "# train_vects = [text_to_array(X_text) for X_text in tqdm(train_df[\"question_text\"])]\n",
        "val_vects = np.array([text_to_array(X_text) for X_text in tqdm(val_df[\"question_text\"][:3000])])\n",
        "val_y = np.array(val_df[\"target\"][:3000])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3000/3000 [00:00<00:00, 4810.59it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "0c950448e0717eaebb920d93cfdc6b4561e21853",
        "execution": {
          "iopub.status.busy": "2021-10-29T14:31:44.722138Z",
          "iopub.execute_input": "2021-10-29T14:31:44.722378Z",
          "iopub.status.idle": "2021-10-29T14:31:44.729759Z",
          "shell.execute_reply.started": "2021-10-29T14:31:44.722323Z",
          "shell.execute_reply": "2021-10-29T14:31:44.728661Z"
        },
        "trusted": true,
        "id": "WPeniepIle_e"
      },
      "source": [
        "# Data providers\n",
        "batch_size = 128\n",
        "\n",
        "def batch_gen(train_df):\n",
        "    n_batches = math.ceil(len(train_df) / batch_size)\n",
        "    while True: \n",
        "        train_df = train_df.sample(frac=1.)  # Shuffle the data.\n",
        "        for i in range(n_batches):\n",
        "            texts = train_df.iloc[i*batch_size:(i+1)*batch_size, 1]\n",
        "            text_arr = np.array([text_to_array(text) for text in texts])\n",
        "            yield text_arr, np.array(train_df[\"target\"][i*batch_size:(i+1)*batch_size])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "706c0224b6112e5a8f00ad25f35e90fbb9519a5f",
        "id": "dB2Ae9DXle_g"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "798c303ec834fb530a60a1e590cfbd9a86f93fde",
        "execution": {
          "iopub.status.busy": "2021-10-29T14:31:44.730949Z",
          "iopub.execute_input": "2021-10-29T14:31:44.731765Z",
          "iopub.status.idle": "2021-10-29T14:31:44.740506Z",
          "shell.execute_reply.started": "2021-10-29T14:31:44.731418Z",
          "shell.execute_reply": "2021-10-29T14:31:44.739545Z"
        },
        "trusted": true,
        "id": "ltCJQZK7le_h"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import CuDNNLSTM, Dense, Bidirectional\n",
        "import tensorflow as tf\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import Bidirectional"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-10-29T14:31:44.741493Z",
          "iopub.execute_input": "2021-10-29T14:31:44.741741Z",
          "iopub.status.idle": "2021-10-29T14:31:44.753795Z",
          "shell.execute_reply.started": "2021-10-29T14:31:44.741687Z",
          "shell.execute_reply": "2021-10-29T14:31:44.752981Z"
        },
        "trusted": true,
        "id": "Y5_XBuM2le_i"
      },
      "source": [
        "# model = Sequential()\n",
        "# model.add(Bidirectional(LSTM(20, return_sequences=True), input_shape=(30, 300)))\n",
        "# model.add(Dense(1, activation=\"sigmoid\"))\n",
        "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "9ad418c95b31ab5691d49b72c7c9622ef9ea42cf",
        "execution": {
          "iopub.status.busy": "2021-10-29T14:31:44.754584Z",
          "iopub.execute_input": "2021-10-29T14:31:44.754797Z",
          "iopub.status.idle": "2021-10-29T14:31:46.027463Z",
          "shell.execute_reply.started": "2021-10-29T14:31:44.754754Z",
          "shell.execute_reply": "2021-10-29T14:31:46.026624Z"
        },
        "trusted": true,
        "id": "hs6dFnmAle_j"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Bidirectional(LSTM(64, return_sequences=True),input_shape=(30, 300)))\n",
        "model.add(Bidirectional(LSTM(64)))\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "3f0b703ded691293450beb4ddf2d903d0e08c737",
        "execution": {
          "iopub.status.busy": "2021-10-29T14:33:14.483903Z",
          "iopub.execute_input": "2021-10-29T14:33:14.484419Z",
          "iopub.status.idle": "2021-10-29T14:49:58.579724Z",
          "shell.execute_reply.started": "2021-10-29T14:33:14.484195Z",
          "shell.execute_reply": "2021-10-29T14:49:58.579031Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5_TJ992le_k",
        "outputId": "ba52b2f3-ab42-43e7-ac0c-86a05abda3c5"
      },
      "source": [
        "mg = batch_gen(train_df)\n",
        "model.fit_generator(mg, epochs=10,\n",
        "                    steps_per_epoch=1000,\n",
        "                    validation_data=(val_vects, val_y),\n",
        "                    verbose=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1000/1000 [==============================] - 257s 249ms/step - loss: 0.1337 - accuracy: 0.9481 - val_loss: 0.1321 - val_accuracy: 0.9453\n",
            "Epoch 2/10\n",
            "1000/1000 [==============================] - 249s 249ms/step - loss: 0.1198 - accuracy: 0.9528 - val_loss: 0.1236 - val_accuracy: 0.9500\n",
            "Epoch 3/10\n",
            "1000/1000 [==============================] - 248s 248ms/step - loss: 0.1154 - accuracy: 0.9544 - val_loss: 0.1249 - val_accuracy: 0.9490\n",
            "Epoch 4/10\n",
            "1000/1000 [==============================] - 245s 245ms/step - loss: 0.1126 - accuracy: 0.9554 - val_loss: 0.1176 - val_accuracy: 0.9530\n",
            "Epoch 5/10\n",
            "1000/1000 [==============================] - 246s 246ms/step - loss: 0.1112 - accuracy: 0.9561 - val_loss: 0.1199 - val_accuracy: 0.9507\n",
            "Epoch 6/10\n",
            "1000/1000 [==============================] - 246s 246ms/step - loss: 0.1082 - accuracy: 0.9571 - val_loss: 0.1157 - val_accuracy: 0.9553\n",
            "Epoch 7/10\n",
            "1000/1000 [==============================] - 245s 245ms/step - loss: 0.1058 - accuracy: 0.9578 - val_loss: 0.1125 - val_accuracy: 0.9567\n",
            "Epoch 8/10\n",
            "1000/1000 [==============================] - 242s 242ms/step - loss: 0.1089 - accuracy: 0.9571 - val_loss: 0.1107 - val_accuracy: 0.9580\n",
            "Epoch 9/10\n",
            "1000/1000 [==============================] - 245s 245ms/step - loss: 0.1060 - accuracy: 0.9578 - val_loss: 0.1064 - val_accuracy: 0.9540\n",
            "Epoch 10/10\n",
            "1000/1000 [==============================] - 246s 246ms/step - loss: 0.1012 - accuracy: 0.9595 - val_loss: 0.1073 - val_accuracy: 0.9557\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f22e393ca50>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "d6cdda0e301be0b84e826e29f0f3a85c41aa6da9",
        "id": "x-_AL08tle_n"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-10-29T14:58:41.653519Z",
          "iopub.execute_input": "2021-10-29T14:58:41.653828Z",
          "iopub.status.idle": "2021-10-29T14:58:42.065104Z",
          "shell.execute_reply.started": "2021-10-29T14:58:41.653773Z",
          "shell.execute_reply": "2021-10-29T14:58:42.064276Z"
        },
        "trusted": true,
        "id": "mM2-xBc3le_n"
      },
      "source": [
        "model.save(\"BLSTM.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-10-29T15:01:16.670564Z",
          "iopub.execute_input": "2021-10-29T15:01:16.673193Z",
          "iopub.status.idle": "2021-10-29T15:01:16.681466Z",
          "shell.execute_reply.started": "2021-10-29T15:01:16.673134Z",
          "shell.execute_reply": "2021-10-29T15:01:16.679554Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3NoRBTlle_p",
        "outputId": "939830f5-8f8d-4028-e33d-bab56439208c"
      },
      "source": [
        "!pip install tensorflowjs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflowjs\n",
            "  Downloading tensorflowjs-3.11.0-py3-none-any.whl (64 kB)\n",
            "\u001b[?25l\r\u001b[K     |█████                           | 10 kB 27.9 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 20 kB 34.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 30 kB 37.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 40 kB 21.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 51 kB 16.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 61 kB 14.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 64 kB 2.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow<3,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflowjs) (2.6.0)\n",
            "Requirement already satisfied: six<2,>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflowjs) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-hub<0.13,>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflowjs) (0.12.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (3.7.4.3)\n",
            "Requirement already satisfied: clang~=5.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (5.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (0.37.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (2.6.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (0.2.0)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (3.1.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (3.3.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.12.1)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.19.5)\n",
            "Requirement already satisfied: keras~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (2.6.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (0.12.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.6.3)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.1.2)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (3.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.37.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.41.0)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (0.4.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.12)\n",
            "Requirement already satisfied: tensorflow-estimator~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (2.6.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow<3,>=2.1.0->tensorflowjs) (1.5.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (1.35.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (3.3.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (1.8.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (4.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (2021.5.30)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.6->tensorflow<3,>=2.1.0->tensorflowjs) (3.6.0)\n",
            "Installing collected packages: tensorflowjs\n",
            "Successfully installed tensorflowjs-3.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-10-29T15:00:00.707577Z",
          "iopub.execute_input": "2021-10-29T15:00:00.707878Z",
          "iopub.status.idle": "2021-10-29T15:00:00.730373Z",
          "shell.execute_reply.started": "2021-10-29T15:00:00.707825Z",
          "shell.execute_reply": "2021-10-29T15:00:00.729374Z"
        },
        "trusted": true,
        "id": "OefSgR5ble_p"
      },
      "source": [
        "import tensorflowjs as tfjs\n",
        "tfjs.converters.save_keras_model(model, \"tfjs_model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5n-p99JRsbcf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXrSEemWJaQW"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.models.load_model(\"/content/BLSTM.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ipOZTmsaq6I"
      },
      "source": [
        "a_file = open(\"/content/gdrive/MyDrive/data_embed/data1.pkl\", \"rb\")\n",
        "output = pickle. load(a_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OG94RB44uGY1",
        "outputId": "e979ff71-e84c-4cd0-b866-7131a61ec2f4"
      },
      "source": [
        "!pip install --upgrade Werkzeug -q"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |█▏                              | 10 kB 19.5 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 20 kB 22.8 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 30 kB 23.8 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 40 kB 24.6 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 51 kB 16.7 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 61 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |████████                        | 71 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 81 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 92 kB 12.9 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 102 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 112 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 122 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 133 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 143 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 153 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 163 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 174 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 184 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 194 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 204 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 215 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 225 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 235 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 245 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 256 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 266 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 276 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 286 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 288 kB 14.0 MB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "flask 1.1.4 requires Werkzeug<2.0,>=0.15, but you have werkzeug 2.0.2 which is incompatible.\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTKiwUoeursg",
        "outputId": "3ca99032-4503-4ef3-833f-d1b8a208f7ad"
      },
      "source": [
        "!python -V"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.7.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOWr8vBXuvW1",
        "outputId": "e9326ef3-44a9-4fdc-a504-2a1a9bcc36ce"
      },
      "source": [
        "!pip install functools"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting functools\n",
            "  Using cached functools-0.5.tar.gz (4.9 kB)\n",
            "Building wheels for collected packages: functools\n",
            "  Building wheel for functools (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for functools\u001b[0m\n",
            "\u001b[?25h  Running setup.py clean for functools\n",
            "Failed to build functools\n",
            "Installing collected packages: functools\n",
            "    Running setup.py install for functools ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31mERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-hw_uibl6/functools_5c393e181852434d941e35b124ba8b79/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-hw_uibl6/functools_5c393e181852434d941e35b124ba8b79/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-hg9kepa6/install-record.txt --single-version-externally-managed --compile --install-headers /usr/local/include/python3.7/functools Check the logs for full command output.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "bUHw2Iu2tsB2",
        "outputId": "b4907a05-6611-4da5-916c-07cb6dc84e78"
      },
      "source": [
        "from werkzeug.utils import cached_property\n",
        "@cached_property\n",
        "def get_embeds():\n",
        "  a_file = open(\"/content/gdrive/MyDrive/data_embed/data1.pkl\", \"rb\")\n",
        "  output = pickle. load(a_file)\n",
        "  return output\n",
        "\n",
        "op = get_embeds()\n",
        "\n",
        "cache = dict()\n",
        "\n",
        "def get_article(url):\n",
        "\n",
        "  print(\"Getting article...\")\n",
        "  if url not in cache:\n",
        "    cache[url] = get_article_from_server(url)\n",
        "\n",
        "  return cache[url]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-e5dfd5925f5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_embeds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'cached_property' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAjQk758DRES"
      },
      "source": [
        "from random import randint\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "from binodcli.binodfile import binodfunc\n",
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.models.load_model(\"/content/BLSTM.h5\")\n",
        "binodfunc('https://drive.google.com/file/d/1yVcCs6QE2EAfbiq-vbWjn4BEGp89E1h7/view?usp=sharing')\n",
        "a_file = open(\"/content/gdrive/MyDrive/data_embed/data1.pkl\", \"rb\") # path of the downloaded file\n",
        "output = pickle. load(a_file)\n",
        "batch_size = 1\n",
        "\n",
        "\n",
        "def take_data():\n",
        "  value = randint(0, 10)\n",
        "  # print(value)\n",
        "\n",
        "  # initialize list of lists\n",
        "  txt = input()\n",
        "  data = [[str(value), txt]]\n",
        "  # Create the pandas DataFrame\n",
        "  df = pd.DataFrame(data, columns = ['qid', 'question_text'])\n",
        "  # df.head()\n",
        "  df.to_csv(\"demo.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHtVwiD7GjB4"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "def text_to_array(text):\n",
        "    empyt_emb = np.zeros(300)\n",
        "    text = text[:-1].split()[:30]\n",
        "    embeds = [output.get(x, empyt_emb) for x in text]\n",
        "    embeds+= [empyt_emb] * (30 - len(embeds))\n",
        "    return np.array(embeds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "b58cd95254f41e5002a17de0c3feab54a5fc3c67",
        "execution": {
          "iopub.status.busy": "2021-10-29T14:32:59.108049Z",
          "iopub.status.idle": "2021-10-29T14:32:59.108499Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sxz-JsT5le_q",
        "outputId": "b28f9f19-db8f-4c01-e024-0adcc678d792"
      },
      "source": [
        "def batch_gen(test_df):\n",
        "    n_batches = math.ceil(len(test_df) / batch_size)\n",
        "\n",
        "    for i in range(n_batches):\n",
        "        # texts = test_df.iloc[i*batch_size:(i+1)*batch_size, 1]\n",
        "        # print(texts)\n",
        "        texts = np.array(df[\"question_text\"])\n",
        "        text_arr = np.array([text_to_array(text) for text in texts])\n",
        "        # print(text_arr[0])\n",
        "        text_arr.shape\n",
        "        yield text_arr\n",
        "\n",
        "def predict_op(path = \"/content/demo.csv\"):\n",
        "  test_df = pd.read_csv(path)\n",
        "  all_preds = []\n",
        "  for x in tqdm(batch_gen(test_df)):\n",
        "    all_preds.extend(model.predict(x).flatten())\n",
        "  return int(all_preds)\n",
        "\n",
        "\n",
        "def main():\n",
        "  take_data()\n",
        "  print(predict_op())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1it [00:00,  4.01it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PKS8GVnK_lG",
        "outputId": "3f0b2972-bcaf-452d-8966-936697f70c63"
      },
      "source": [
        "print(all_preds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.719097]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTqNRaxD9k4b",
        "outputId": "ae3fb03a-d125-455f-c478-1a42f070790b"
      },
      "source": [
        "df = pd.read_csv(\"/content/demo.csv\")\n",
        "batch_size = 1\n",
        "text = df[\"question_text\"][0]\n",
        "tt = text[:-1].split()[:30]\n",
        "# print(tt)\n",
        "# model.predict(text_to_array(text)).flatten()\n",
        "# model.predict(text_to_array(text))\n",
        "# print(text)\n",
        "# type(df[\"question_text\"][0])\n",
        "# df.head()\n",
        "n_batches = math.ceil(len(df) / batch_size)\n",
        "for i in range(n_batches):\n",
        "  texts = np.array(df[\"question_text\"])\n",
        "  text_arr = np.array([text_to_array(text) for text in texts])\n",
        "\n",
        "  print(text_arr)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[ 0.17117     0.43757001 -0.19373    ...  0.11892     0.29796001\n",
            "   -0.16007   ]\n",
            "  [-0.013048    0.066305   -0.18674999 ... -0.53574997  0.34551999\n",
            "    0.038396  ]\n",
            "  [ 0.19410001  0.22603001 -0.43764001 ...  0.091957    0.38631999\n",
            "    0.11736   ]\n",
            "  ...\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]]\n",
            "\n",
            " [[-0.31887001  0.1944     -0.11692    ...  0.22766     0.4824\n",
            "    0.35543001]\n",
            "  [-0.014531   -0.071761   -0.59626001 ... -0.12443     0.63753998\n",
            "    0.12262   ]\n",
            "  [ 0.030971    0.24589001 -0.2484     ... -0.23329     0.11165\n",
            "    0.088833  ]\n",
            "  ...\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]]]\n",
            "[[[ 0.17117     0.43757001 -0.19373    ...  0.11892     0.29796001\n",
            "   -0.16007   ]\n",
            "  [-0.013048    0.066305   -0.18674999 ... -0.53574997  0.34551999\n",
            "    0.038396  ]\n",
            "  [ 0.19410001  0.22603001 -0.43764001 ...  0.091957    0.38631999\n",
            "    0.11736   ]\n",
            "  ...\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]]\n",
            "\n",
            " [[-0.31887001  0.1944     -0.11692    ...  0.22766     0.4824\n",
            "    0.35543001]\n",
            "  [-0.014531   -0.071761   -0.59626001 ... -0.12443     0.63753998\n",
            "    0.12262   ]\n",
            "  [ 0.030971    0.24589001 -0.2484     ... -0.23329     0.11165\n",
            "    0.088833  ]\n",
            "  ...\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]\n",
            "  [ 0.          0.          0.         ...  0.          0.\n",
            "    0.        ]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "3e6ed54def110c881f401c6f8a844752baedcfbd",
        "execution": {
          "iopub.status.busy": "2021-10-29T14:32:59.109219Z",
          "iopub.status.idle": "2021-10-29T14:32:59.109663Z"
        },
        "trusted": true,
        "id": "xdrlDtV3le_q"
      },
      "source": [
        "\n",
        "y_te = (np.array(all_preds) > 0.5).astype(np.int)\n",
        "\n",
        "submit_df = pd.DataFrame({\"qid\": test_df[\"qid\"], \"prediction\": y_te})\n",
        "submit_df.to_csv(\"submission.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-10-29T14:32:59.110364Z",
          "iopub.status.idle": "2021-10-29T14:32:59.110828Z"
        },
        "trusted": true,
        "id": "IHegFdWTle_r"
      },
      "source": [
        "# import tensorflowjs as tfjs\n",
        "# tfjs.converters.save_keras_model(model, tfjs_target_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orCHn9z38MNV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFvn3RNgle_r"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}